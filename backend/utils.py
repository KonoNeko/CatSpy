import re
from urllib.parse import urlparse
from difflib import SequenceMatcher

URGENCY_KEYWORDS = ["urgent", "immediately", "asap", "verify", "verify your", "action required", "limited time", "secure your account"]
CREDENTIAL_KEYWORDS = ["password", "passcode", "verify your account", "login", "credit card", "social security", "ssn", "pin"]
SHORTENER_DOMAINS = {"bit.ly","tinyurl.com","t.co","goo.gl","ow.ly","is.gd","buff.ly"}

# å“ç‰ŒåŠå…¶åˆæ³•åŸŸå
BRAND_DOMAINS = {
    "paypal": ["paypal.com", "paypal.co.uk", "paypal.ca", "paypal.de", "paypal.fr"],
    "amazon": ["amazon.com", "amazon.co.uk", "amazon.ca", "amazon.de", "amazon.fr", "amazon.in"],
    "google": ["google.com", "gmail.com", "youtube.com", "google.co.uk", "google.ca"],
    "microsoft": ["microsoft.com", "outlook.com", "hotmail.com", "live.com", "office.com", "xbox.com"],
    "apple": ["apple.com", "icloud.com", "itunes.com", "me.com"],
    "netflix": ["netflix.com"],
    "facebook": ["facebook.com", "fb.com", "messenger.com"],
    "instagram": ["instagram.com"],
    "linkedin": ["linkedin.com"],
    "twitter": ["twitter.com", "x.com"],
    "whatsapp": ["whatsapp.com"],
    "bank": ["bankofamerica.com", "chase.com", "wellsfargo.com", "citibank.com"]
}

# ç™½åå•ï¼šå¸¸è§åˆæ³•ç½‘ç«™ï¼ˆåŒ…æ‹¬å„ç§åŸŸåå½¢å¼ï¼‰
WHITELIST_DOMAINS = {
    # æœç´¢å¼•æ“
    "google.com", "bing.com", "yahoo.com", "duckduckgo.com", "baidu.com",
    # ç¤¾äº¤åª’ä½“
    "facebook.com", "fb.com", "instagram.com", "twitter.com", "x.com", "linkedin.com",
    "reddit.com", "pinterest.com", "tumblr.com", "snapchat.com", "tiktok.com",
    "whatsapp.com", "telegram.org", "discord.com", "slack.com",
    # ç”µå•†
    "amazon.com", "ebay.com", "alibaba.com", "aliexpress.com", "etsy.com",
    "walmart.com", "target.com", "bestbuy.com", "costco.com",
    # ç§‘æŠ€å…¬å¸
    "microsoft.com", "apple.com", "google.com", "ibm.com", "oracle.com",
    "adobe.com", "salesforce.com", "zoom.us", "dropbox.com",
    # é‚®ç®±æœåŠ¡
    "gmail.com", "outlook.com", "hotmail.com", "yahoo.com", "protonmail.com",
    "icloud.com", "mail.ru", "aol.com", "live.com",
    # æµåª’ä½“
    "netflix.com", "youtube.com", "hulu.com", "disneyplus.com", "spotify.com",
    "twitch.tv", "vimeo.com", "soundcloud.com",
    # æ–°é—»åª’ä½“
    "cnn.com", "bbc.com", "nytimes.com", "wsj.com", "theguardian.com",
    "forbes.com", "reuters.com", "bloomberg.com",
    # é“¶è¡Œé‡‘è
    "paypal.com", "stripe.com", "square.com", "venmo.com",
    "bankofamerica.com", "chase.com", "wellsfargo.com", "citibank.com",
    # å¼€å‘è€…å·¥å…·
    "github.com", "gitlab.com", "stackoverflow.com", "npmjs.com",
    "pypi.org", "docker.com", "heroku.com", "vercel.com",
    # äº‘æœåŠ¡
    "aws.amazon.com", "azure.microsoft.com", "cloud.google.com",
    "digitalocean.com", "cloudflare.com", "linode.com",
    # å­¦ä¹ æ•™è‚²
    "wikipedia.org", "coursera.org", "udemy.com", "khanacademy.org",
    "edx.org", "linkedin.com", "medium.com",
    # å…¶ä»–å¸¸è§ç½‘ç«™
    "wordpress.com", "godaddy.com", "wix.com", "squarespace.com",
    "shopify.com", "mailchimp.com", "canva.com", "notion.so",
    # ä¸­å›½å¸¸è§ç½‘ç«™
    "taobao.com", "tmall.com", "jd.com", "weibo.com", "wechat.com",
    "qq.com", "163.com", "126.com", "sina.com", "sohu.com",
    # å„å›½åŸŸåå˜ä½“
    "amazon.co.uk", "amazon.de", "amazon.fr", "amazon.in", "amazon.jp",
    "google.co.uk", "google.de", "google.fr", "google.ca", "google.com.au",
    "ebay.co.uk", "ebay.de", "ebay.fr", "ebay.ca", "ebay.com.au",
    "paypal.co.uk", "paypal.de", "paypal.fr", "paypal.ca",
    "netflix.co.uk", "netflix.ca", "netflix.com.au",
    "bbc.co.uk", "bbc.com",
}

def similarity_ratio(a, b):
    """è®¡ç®—ä¸¤ä¸ªå­—ç¬¦ä¸²çš„ç›¸ä¼¼åº¦ (0.0-1.0)"""
    return SequenceMatcher(None, a.lower(), b.lower()).ratio()

def is_whitelisted(domain):
    """
    æ£€æŸ¥åŸŸåæ˜¯å¦åœ¨ç™½åå•ä¸­
    æ”¯æŒå¤šç§æ ¼å¼ï¼š
    - å®Œæ•´åŒ¹é…: example.com
    - å­åŸŸå: api.example.com, www.example.com
    """
    if not domain:
        return False
    
    # ç§»é™¤www.å‰ç¼€å’Œç«¯å£å·ï¼Œè½¬æ¢ä¸ºå°å†™
    clean_domain = domain.replace("www.", "").split(":")[0].lower()
    
    # ç›´æ¥åŒ¹é…
    if clean_domain in WHITELIST_DOMAINS:
        return True
    
    # æ£€æŸ¥æ˜¯å¦æ˜¯ç™½åå•åŸŸåçš„å­åŸŸå
    # ä¾‹å¦‚ï¼šapi.github.com åº”è¯¥åŒ¹é… github.com
    for whitelist_domain in WHITELIST_DOMAINS:
        if clean_domain.endswith("." + whitelist_domain) or clean_domain == whitelist_domain:
            return True
    
    # æ£€æŸ¥BRAND_DOMAINSä¸­çš„åˆæ³•åŸŸå
    for brand_domains in BRAND_DOMAINS.values():
        for legit_domain in brand_domains:
            if clean_domain.endswith(legit_domain) or clean_domain == legit_domain:
                return True
    
    return False

def check_brand_spoofing(domain, text_lower):
    """
    æ£€æµ‹å“ç‰Œä¼ªé€ ï¼ŒåŒ…æ‹¬ï¼š
    1. åŸŸåä¸çŸ¥åå“ç‰Œé«˜åº¦ç›¸ä¼¼ï¼ˆæ‹¼å†™é”™è¯¯ï¼‰
    2. æ–‡æœ¬ä¸­æåˆ°å“ç‰Œä½†åŸŸåä¸åŒ¹é…
    3. åŒ…å«å“ç‰Œåä½œä¸ºå­ä¸²ï¼ˆä¾‹å¦‚ï¼šmmmmmicroft åŒ…å« microftï¼‰
    4. é€šè¿‡é‡å¤å­—ç¬¦æ··æ·†ï¼ˆä¾‹å¦‚ï¼šgooogle, micccrosoftï¼‰
    
    é‡è¦ï¼šç™½åå•åŸŸåä¸ä¼šè¢«æ ‡è®°ä¸ºä¼ªé€ 
    """
    if not domain:
        return "none", None
    
    # ç§»é™¤www.å‰ç¼€å’Œç«¯å£å·
    clean_domain = domain.replace("www.", "").split(":")[0].lower()
    
    # ğŸ”¥ ç™½åå•æ£€æŸ¥ - å¦‚æœåœ¨ç™½åå•ä¸­ï¼Œç›´æ¥è¿”å›å®‰å…¨
    if is_whitelisted(clean_domain):
        return "none", None
    
    for brand, legitimate_domains in BRAND_DOMAINS.items():
        # æ£€æŸ¥æ–‡æœ¬ä¸­æ˜¯å¦æåˆ°è¯¥å“ç‰Œ
        brand_mentioned = brand in text_lower
        
        # æ£€æŸ¥åŸŸåæ˜¯å¦æ˜¯åˆæ³•åŸŸå
        is_legitimate = any(clean_domain.endswith(legit) for legit in legitimate_domains)
        
        if brand_mentioned and not is_legitimate:
            # å“ç‰Œåœ¨æ–‡æœ¬ä¸­ä½†åŸŸåä¸æ˜¯å®˜æ–¹çš„ - å¯èƒ½ä¼ªé€ 
            return "suspected", brand
        
        # æ£€æŸ¥åŸŸåä¸å“ç‰Œåçš„ç›¸ä¼¼åº¦ï¼ˆæ‹¼å†™é”™è¯¯æ£€æµ‹ï¼‰
        # ä¾‹å¦‚: mmicroft.com vs microsoft
        domain_parts = clean_domain.split('.')
        for part in domain_parts:
            if len(part) >= 4:  # åªæ£€æŸ¥è¶³å¤Ÿé•¿çš„éƒ¨åˆ†
                # ğŸ”¥ æ–°å¢ï¼šæ£€æŸ¥æ˜¯å¦åŒ…å«å“ç‰Œåä½œä¸ºå­ä¸²
                # ä¾‹å¦‚ï¼šmmmmmicroft åŒ…å«ç±»ä¼¼ microsoft çš„éƒ¨åˆ†
                if brand in part and not is_legitimate:
                    return "likely", f"{brand} (embedded in: {part})"
                
                # ğŸ”¥ æ–°å¢ï¼šå»é™¤é‡å¤å­—ç¬¦åå†æ£€æŸ¥
                # ä¾‹å¦‚ï¼šgooogle -> gogle, micccrosoft -> microsoft
                dedupe_part = re.sub(r'(.)\1+', r'\1', part)  # å°†è¿ç»­é‡å¤å­—ç¬¦æ›¿æ¢ä¸ºå•ä¸ª
                if dedupe_part != part:  # å¦‚æœæœ‰é‡å¤å­—ç¬¦
                    dedupe_sim = similarity_ratio(dedupe_part, brand)
                    if dedupe_sim >= 0.7:
                        return "likely", f"{brand} (char-stuffing: {part})"
                
                # ğŸ”¥ æ–°å¢ï¼šæ£€æŸ¥å“ç‰Œåæ˜¯å¦ä½œä¸ºå­ä¸²å­˜åœ¨ï¼ˆæ¨¡ç³ŠåŒ¹é…ï¼‰
                # ä¾‹å¦‚ï¼šmmmmmicroft ä¸ microsoft çš„æœ€é•¿å…¬å…±å­åºåˆ—
                for i in range(len(part) - len(brand) + 1):
                    substring = part[i:i+len(brand)]
                    if similarity_ratio(substring, brand) >= 0.75:
                        return "likely", f"{brand} (substring match: {part})"
                
                # åŸæœ‰çš„ç›¸ä¼¼åº¦æ£€æµ‹
                sim = similarity_ratio(part, brand)
                # ç›¸ä¼¼åº¦åœ¨70%-95%ä¹‹é—´ - å¾ˆå¯èƒ½æ˜¯æ‹¼å†™é”™è¯¯çš„å“ç‰Œä¼ªé€ 
                if 0.7 <= sim < 0.95:
                    return "likely", f"{brand} (typo: {part})"
                # å®Œå…¨åŒ¹é…ä½†ä¸åœ¨åˆæ³•åŸŸååˆ—è¡¨ - ç¡®è®¤ä¼ªé€ 
                elif sim >= 0.95 and not is_legitimate:
                    return "confirmed", brand
    
    return "none", None

def extract_urls(text):
    url_re = re.compile(r'(https?://[^\s]+)')
    return url_re.findall(text)

def domain_from_url(url):
    try:
        p = urlparse(url)
        return p.netloc.lower()
    except:
        return ""

def check_heuristics(text):
    txt = text.lower()
    cues = []
    heuristics = {
        "domain_risk":"ok",
        "urgency_language":"none",
        "creds_request":"none",
        "shortener_obfuscation":"none",
        "brand_spoof":"none"
    }

    urls = extract_urls(text)
    spoofed_brand = None
    
    for u in urls:
        dom = domain_from_url(u)
        if dom:
            # æ£€æŸ¥å“ç‰Œä¼ªé€ 
            spoof_level, brand_info = check_brand_spoofing(dom, txt)
            if spoof_level != "none":
                heuristics["brand_spoof"] = spoof_level
                spoofed_brand = brand_info
                cues.append({"type":"brand_spoof","text":f"{dom} (mimics {brand_info})","details":{"domain":dom, "brand":brand_info}})
            
            # æ£€æŸ¥çŸ­é“¾æ¥
            if dom in SHORTENER_DOMAINS:
                heuristics["shortener_obfuscation"] = "present"
                cues.append({"type":"url","text":u,"details":{"domain":dom}})
            
            # æ£€æŸ¥å¯ç–‘åŸŸåï¼ˆIPåœ°å€æˆ–åŒ…å«å¤§é‡æ•°å­—ï¼‰
            if re.search(r'\d{3,}', dom) or re.match(r'^\d+\.\d+\.\d+\.\d+$', dom):
                heuristics["domain_risk"] = "suspicious"
                cues.append({"type":"url","text":u,"details":{"domain":dom}})

    urgency_hits = [k for k in URGENCY_KEYWORDS if k in txt]
    if urgency_hits:
        heuristics["urgency_language"] = "high" if any(k in txt for k in ["urgent","immediately","action required","asap"]) else "low"
        cues.append({"type":"keyword","text":", ".join(set(urgency_hits)),"details":{}})

    cred_hits = [k for k in CREDENTIAL_KEYWORDS if k in txt]
    if cred_hits:
        heuristics["creds_request"] = "confirmed" if any(k in txt for k in ["password","login","verify your account"]) else "maybe"
        cues.append({"type":"credential_request","text":", ".join(set(cred_hits)),"details":{}})

    # æ£€æŸ¥æ–‡æœ¬ä¸­æåˆ°çš„å“ç‰Œï¼ˆå¦‚æœåŸŸåæ²¡æœ‰æ£€æµ‹åˆ°å“ç‰Œä¼ªé€ ï¼‰
    if heuristics["brand_spoof"] == "none":
        for brand in BRAND_DOMAINS.keys():
            if brand in txt:
                heuristics["brand_spoof"] = "possible"
                cues.append({"type":"brand","text":brand,"details":{}})
                break

    return heuristics, cues

def map_model_to_frontend(model_result, text, model_name="distilbert-base-uncased"):
    probs = model_result.get("scores", [])
    safe_prob = float(probs[0]) if len(probs) > 0 else 0.5
    phishing_prob = float(probs[1]) if len(probs) > 1 else 0.5
    
    pred_label = int(model_result.get("label", 0))
    heuristics, cues = check_heuristics(text)
    
    urls = extract_urls(text)
    has_url = len(urls) > 0
    has_http = 'http://' in text.lower() or 'https://' in text.lower()
    
    threat_keywords = ['password', 'login', 'account', 'verify', 'bank', 'paypal', 'amazon', 
                      'suspended', 'locked', 'confirm', 'update', 'click', 'urgent', 'security']
    has_threat_keywords = any(word in text.lower() for word in threat_keywords)
    
    has_meaningful_words = len([w for w in text.split() if len(w) > 3 and w.isalpha()]) >= 3
    
    is_gibberish = (not has_url and not has_http and 
                    not has_threat_keywords and
                    (not has_meaningful_words or len(text) < 20))
    
    has_strong_heuristics = (
        heuristics["creds_request"] in ["confirmed", "maybe"] or
        heuristics["urgency_language"] == "high" or
        heuristics["domain_risk"] in ["suspicious", "high"] or
        heuristics["shortener_obfuscation"] == "present"
    )

    if is_gibberish:
        base = min(25, phishing_prob * 30)
        weight_map = {
            "domain_risk": {"ok":0, "suspicious":0, "high":0},
            "urgency_language": {"none":0, "low":0, "high":0},
            "creds_request": {"none":0, "maybe":0, "confirmed":0},
            "shortener_obfuscation": {"none":0, "present":0},
            "brand_spoof": {"none":0, "possible":0, "likely":0}
        }
    elif has_strong_heuristics:
        if has_url or has_http:
            base = 50 + (phishing_prob - 0.3) * 80
        else:
            base = 45 + max(0, (phishing_prob - 0.3) * 60)
        
        weight_map = {
            "domain_risk": {"ok":0, "suspicious":12, "high":18},
            "urgency_language": {"none":0, "low":8, "high":15},
            "creds_request": {"none":0, "maybe":12, "confirmed":18},
            "shortener_obfuscation": {"none":0, "present":12},
            "brand_spoof": {"none":0, "possible":10, "likely":15}
        }
    elif pred_label == 0:
        base = max(0, (phishing_prob - 0.5) * 100)
        
        weight_map = {
            "domain_risk": {"ok":0, "suspicious":5, "high":12},
            "urgency_language": {"none":0, "low":3, "high":8},
            "creds_request": {"none":0, "maybe":5, "confirmed":10},
            "shortener_obfuscation": {"none":0, "present":5},
            "brand_spoof": {"none":0, "possible":3, "likely":8}
        }
    else:
        if not has_url and not has_http and not has_threat_keywords:
            base = min(40, 25 + (phishing_prob - 0.5) * 30)
        else:
            base = 50 + (phishing_prob - 0.5) * 100
        
        if has_url or has_http:
            weight_map = {
                "domain_risk": {"ok":0, "suspicious":10, "high":15},
                "urgency_language": {"none":0, "low":8, "high":12},
                "creds_request": {"none":0, "maybe":10, "confirmed":15},
                "shortener_obfuscation": {"none":0, "present":10},
                "brand_spoof": {"none":0, "possible":8, "likely":12}
            }
        else:
            weight_map = {
                "domain_risk": {"ok":0, "suspicious":3, "high":8},
                "urgency_language": {"none":0, "low":5, "high":10},
                "creds_request": {"none":0, "maybe":8, "confirmed":12},
                "shortener_obfuscation": {"none":0, "present":5},
                "brand_spoof": {"none":0, "possible":5, "likely":10}
            }

    extra = 0
    for k,v in heuristics.items():
        extra += weight_map.get(k, {}).get(v, 0)

    score = int(round(min(max(base + extra, 0), 100)))

    if score >= 70:
        result = "PHISHING"
    elif score >= 35:
        result = "SUSPICIOUS"
    else:
        result = "SAFE"

    summary_parts = []
    risk_indicators = []
    
    if heuristics["creds_request"] != "none":
        summary_parts.append("Credential request detected")
        risk_indicators.append({"type": "credential", "severity": "high", "text": "Requests sensitive information"})
    if heuristics["urgency_language"] != "none":
        summary_parts.append("Urgent language detected")
        risk_indicators.append({"type": "urgency", "severity": "medium", "text": "Uses pressure tactics"})
    if heuristics["domain_risk"] != "ok":
        summary_parts.append("Suspicious domain")
        risk_indicators.append({"type": "domain", "severity": "high", "text": "Untrusted or suspicious domain"})
    if heuristics["shortener_obfuscation"] != "none":
        risk_indicators.append({"type": "obfuscation", "severity": "medium", "text": "Uses URL shortener"})
    if heuristics["brand_spoof"] != "none":
        risk_indicators.append({"type": "spoofing", "severity": "high", "text": "Possible brand impersonation"})
    
    if not summary_parts:
        summary = "No significant threats detected. Content appears safe."
    else:
        summary = "; ".join(summary_parts) + "."
    
    confidence = int(round(max(safe_prob, phishing_prob) * 100))
    
    return {
        "result": result,
        "score": score,
        "confidence": confidence,
        "summary": summary,
        "risk_indicators": risk_indicators,
        "heuristics": heuristics,
        "cues": cues,
        "details": {
            "has_url": has_url,
            "has_http": has_http,
            "is_gibberish": is_gibberish,
            "url_count": len(urls)
        },
        "model": {
            "name": model_name,
            "prediction": "safe" if pred_label == 0 else "phishing",
            "confidence": {
                "safe": round(safe_prob * 100, 2),
                "phishing": round(phishing_prob * 100, 2)
            }
        }
    }
