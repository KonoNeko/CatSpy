"""
Quick GPU Training Script for Deepfake Detection - 20åˆ†é’Ÿå¿«é€Ÿè®­ç»ƒ
ä½¿ç”¨åˆæˆæ•°æ®è¿›è¡Œå¿«é€Ÿè®­ç»ƒå’Œæµ‹è¯•GPUæ€§èƒ½
"""

import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, TensorDataset
import torchvision.models as models
from pathlib import Path
import time
from datetime import datetime
import json

def create_synthetic_dataset(num_samples=2000, img_size=224):
    """åˆ›å»ºåˆæˆæ•°æ®é›†ç”¨äºå¿«é€Ÿæµ‹è¯•"""
    print(f"ğŸ“¦ ç”Ÿæˆ {num_samples} ä¸ªåˆæˆå›¾ç‰‡æ ·æœ¬...")
    
    # ç”Ÿæˆéšæœºå›¾ç‰‡æ•°æ® (batch, channels, height, width)
    images = torch.randn(num_samples, 3, img_size, img_size)
    # ç”Ÿæˆæ ‡ç­¾ (50% real, 50% fake)
    labels = torch.cat([torch.zeros(num_samples//2), torch.ones(num_samples//2)]).long()
    
    # æ‰“ä¹±æ•°æ®
    perm = torch.randperm(num_samples)
    images = images[perm]
    labels = labels[perm]
    
    return TensorDataset(images, labels)


def create_model(pretrained=True, num_classes=2):
    """åˆ›å»ºResNet-18æ¨¡å‹"""
    print("ğŸ—ï¸ æ„å»ºResNet-18æ¨¡å‹...")
    model = models.resnet18(weights='IMAGENET1K_V1' if pretrained else None)
    num_features = model.fc.in_features
    model.fc = nn.Sequential(
        nn.Dropout(0.5),
        nn.Linear(num_features, num_classes)
    )
    return model


def train_epoch(model, dataloader, criterion, optimizer, device):
    """è®­ç»ƒä¸€ä¸ªepoch"""
    model.train()
    running_loss = 0.0
    correct = 0
    total = 0
    
    for batch_idx, (images, labels) in enumerate(dataloader):
        images, labels = images.to(device), labels.to(device)
        
        optimizer.zero_grad()
        outputs = model(images)
        loss = criterion(outputs, labels)
        loss.backward()
        optimizer.step()
        
        running_loss += loss.item()
        _, predicted = torch.max(outputs.data, 1)
        total += labels.size(0)
        correct += (predicted == labels).sum().item()
        
        if batch_idx % 10 == 0:
            print(f"  Batch {batch_idx}/{len(dataloader)} - Loss: {loss.item():.4f} - Acc: {100*correct/total:.2f}%")
    
    epoch_loss = running_loss / len(dataloader)
    epoch_acc = 100 * correct / total
    return epoch_loss, epoch_acc


def validate(model, dataloader, criterion, device):
    """éªŒè¯æ¨¡å‹"""
    model.eval()
    running_loss = 0.0
    correct = 0
    total = 0
    
    with torch.no_grad():
        for images, labels in dataloader:
            images, labels = images.to(device), labels.to(device)
            outputs = model(images)
            loss = criterion(outputs, labels)
            
            running_loss += loss.item()
            _, predicted = torch.max(outputs.data, 1)
            total += labels.size(0)
            correct += (predicted == labels).sum().item()
    
    val_loss = running_loss / len(dataloader)
    val_acc = 100 * correct / total
    return val_loss, val_acc


def main():
    print("\n" + "="*70)
    print("ğŸš€ å¿«é€ŸGPUè®­ç»ƒ - Deepfakeæ£€æµ‹æ¨¡å‹")
    print("="*70)
    
    # é…ç½®
    EPOCHS = 3  # å¿«é€Ÿè®­ç»ƒåªéœ€3ä¸ªepoch
    BATCH_SIZE = 64  # å¢å¤§batch sizeåˆ©ç”¨GPU
    LR = 0.001
    NUM_TRAIN = 1600
    NUM_VAL = 400
    
    # æ£€æŸ¥GPU
    if torch.cuda.is_available():
        device = torch.device('cuda')
        gpu_name = torch.cuda.get_device_name(0)
        print(f"âœ… GPUå¯ç”¨: {gpu_name}")
        print(f"   æ˜¾å­˜: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB")
    else:
        device = torch.device('cpu')
        print("âš ï¸ GPUä¸å¯ç”¨ï¼Œä½¿ç”¨CPUè®­ç»ƒ")
    
    print(f"\nğŸ“Š è®­ç»ƒé…ç½®:")
    print(f"   - Epochs: {EPOCHS}")
    print(f"   - Batch Size: {BATCH_SIZE}")
    print(f"   - Learning Rate: {LR}")
    print(f"   - è®­ç»ƒæ ·æœ¬: {NUM_TRAIN}")
    print(f"   - éªŒè¯æ ·æœ¬: {NUM_VAL}")
    print(f"   - è®¾å¤‡: {device}")
    print()
    
    # åˆ›å»ºåˆæˆæ•°æ®é›†
    start_time = time.time()
    
    train_dataset = create_synthetic_dataset(NUM_TRAIN)
    val_dataset = create_synthetic_dataset(NUM_VAL)
    
    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=0)
    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=0)
    
    print(f"âœ… æ•°æ®é›†åˆ›å»ºå®Œæˆ ({time.time()-start_time:.1f}ç§’)")
    print()
    
    # åˆ›å»ºæ¨¡å‹
    model = create_model(pretrained=True)
    model = model.to(device)
    
    criterion = nn.CrossEntropyLoss()
    optimizer = optim.Adam(model.parameters(), lr=LR, weight_decay=1e-4)
    
    # è®­ç»ƒå¾ªç¯
    print("ğŸ“ å¼€å§‹è®­ç»ƒ...\n")
    history = {'train_loss': [], 'train_acc': [], 'val_loss': [], 'val_acc': []}
    best_val_acc = 0
    
    for epoch in range(EPOCHS):
        epoch_start = time.time()
        print(f"Epoch {epoch+1}/{EPOCHS}")
        print("-" * 70)
        
        # è®­ç»ƒ
        train_loss, train_acc = train_epoch(model, train_loader, criterion, optimizer, device)
        
        # éªŒè¯
        val_loss, val_acc = validate(model, val_loader, criterion, device)
        
        # è®°å½•
        history['train_loss'].append(train_loss)
        history['train_acc'].append(train_acc)
        history['val_loss'].append(val_loss)
        history['val_acc'].append(val_acc)
        
        epoch_time = time.time() - epoch_start
        
        print(f"\nğŸ“ˆ Epoch {epoch+1} ç»“æœ (è€—æ—¶: {epoch_time:.1f}ç§’):")
        print(f"   è®­ç»ƒ - Loss: {train_loss:.4f} | Acc: {train_acc:.2f}%")
        print(f"   éªŒè¯ - Loss: {val_loss:.4f} | Acc: {val_acc:.2f}%")
        
        # ä¿å­˜æœ€ä½³æ¨¡å‹
        if val_acc > best_val_acc:
            best_val_acc = val_acc
            output_dir = Path('deepfake_models')
            output_dir.mkdir(exist_ok=True)
            
            torch.save({
                'epoch': epoch,
                'model_state_dict': model.state_dict(),
                'optimizer_state_dict': optimizer.state_dict(),
                'val_acc': val_acc,
                'val_loss': val_loss,
            }, output_dir / 'resnet18_deepfake.pth')
            print(f"   âœ… ä¿å­˜æœ€ä½³æ¨¡å‹ (éªŒè¯å‡†ç¡®ç‡: {val_acc:.2f}%)")
        
        print()
    
    # æ€»ç»“
    total_time = time.time() - start_time
    print("="*70)
    print("ğŸ‰ è®­ç»ƒå®Œæˆï¼")
    print("="*70)
    print(f"â±ï¸ æ€»è€—æ—¶: {total_time/60:.1f} åˆ†é’Ÿ")
    print(f"ğŸ† æœ€ä½³éªŒè¯å‡†ç¡®ç‡: {best_val_acc:.2f}%")
    print(f"ğŸ“ æ¨¡å‹ä¿å­˜ä½ç½®: deepfake_models/resnet18_deepfake.pth")
    
    # ä¿å­˜è®­ç»ƒå†å²
    output_dir = Path('deepfake_models')
    config = {
        'epochs': EPOCHS,
        'batch_size': BATCH_SIZE,
        'learning_rate': LR,
        'best_val_acc': float(best_val_acc),
        'total_time_minutes': total_time/60,
        'device': str(device),
        'gpu_name': torch.cuda.get_device_name(0) if torch.cuda.is_available() else 'CPU',
        'timestamp': datetime.now().isoformat(),
        'training_mode': 'quick_synthetic_data'
    }
    
    with open(output_dir / 'quick_training_config.json', 'w') as f:
        json.dump(config, f, indent=2)
    
    with open(output_dir / 'quick_training_history.json', 'w') as f:
        json.dump(history, f, indent=2)
    
    print(f"\nâœ… è®­ç»ƒé…ç½®å’Œå†å²å·²ä¿å­˜")
    print("="*70)
    
    # GPUå†…å­˜ä½¿ç”¨æƒ…å†µ
    if torch.cuda.is_available():
        print(f"\nğŸ’¾ GPUæ˜¾å­˜ä½¿ç”¨:")
        print(f"   å·²åˆ†é…: {torch.cuda.memory_allocated(0) / 1024**2:.1f} MB")
        print(f"   å·²ç¼“å­˜: {torch.cuda.memory_reserved(0) / 1024**2:.1f} MB")
    
    print("\nğŸ¯ ä¸‹ä¸€æ­¥: ä½¿ç”¨çœŸå®æ•°æ®é›†é‡æ–°è®­ç»ƒä»¥è·å¾—æ›´å¥½çš„æ•ˆæœ")
    print()


if __name__ == '__main__':
    main()
